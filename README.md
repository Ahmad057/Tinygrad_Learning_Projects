# Tinygrad_Learning_Projects
This repository is dedicated to storing and showcasing my learning projects and files related to Tinygrad

## Parameter Tuning

Notebook (custom_tinygrad.ipynb) in this repository includes parameters:

1. **Hyperparameters**:
   - Learning Rate: 0.01
   - Number of Epochs: 1000 steps per epochs
   - Batch Size: 64


2. **Model-Specific Parameters**:
   - Input Size: 150x150x1
   - Number of Layers: 4 convolutional layers and 2 fully connected layers
   - Activation Functions: ReLu, Sigmoid for output layer 
   - Loss Function: Binary Cross Entropy
   - Optimizer: Adam, SGD

The output.txt file within this repository provides a comprehensive breakdown of the achieved accuracy, loss, and predictions against actual labels for each epoch.

